# Fake Review Detection Project - Comprehensive Report

## Project Overview
This project implements a fake review detection system using machine learning techniques. The goal is to identify deceptive online reviews that may be written by bots, paid reviewers, or competitors to manipulate public opinion. We use two different approaches: a multi-modal deep learning model and a traditional Random Forest classifier.

## What is Fake Review Detection?
Fake review detection is a type of natural language processing (NLP) and machine learning task where we analyze online reviews to determine if they are genuine (written by real customers) or deceptive (written with ulterior motives). This is important because fake reviews can mislead consumers and harm businesses.

## Data Sources

### Yelp Academic Dataset
We use the Yelp Academic Dataset, which is a large collection of real-world data from Yelp.com containing:
- **yelp_academic_dataset_review.json**: Contains millions of reviews with text, ratings (stars), dates, and user/business IDs
- **yelp_academic_dataset_user.json**: Contains user profile information including review history, ratings, social connections, and status

### Data Structure
Each review in the dataset contains:
- **text**: The actual review content
- **stars**: Rating from 1-5 stars
- **user_id**: Unique identifier for the reviewer
- **business_id**: Unique identifier for the business
- **date**: When the review was posted
- **useful**: Number of users who found the review useful
- **funny**: Number of users who found the review funny
- **cool**: Number of users who found the review cool

Each user profile contains:
- **user_id**: Unique user identifier
- **review_count**: Total number of reviews written by this user
- **average_stars**: User's average star rating across all reviews
- **elite**: Years the user had elite status (premium membership)
- **friends**: List of friend user IDs
- **fans**: Number of users who follow this user
- **compliments**: Various types of compliments received (hot, funny, cool, etc.)

## Data Preprocessing

### Step 1: Loading Data
We load a subset of 500,000 reviews from the review JSON file and all user data from the user JSON file.

### Step 2: Feature Extraction
We extract 7 behavioral features from the user data that help identify suspicious patterns:

1. **user_review_count**: Total number of reviews written by this user (from user.review_count)
2. **user_average_stars**: User's average star rating across all their reviews (from user.average_stars)
3. **is_elite**: Binary feature - 1 if user has elite status, 0 otherwise (derived from user.elite field)
4. **review_length**: Number of words in the current review text (calculated as len(text.split()))
5. **user_friends**: Number of friends this user has on Yelp (length of user.friends array)
6. **user_fans**: Number of fans/followers this user has (from user.fans)
7. **user_compliment_count**: Total compliments received (sum of all compliment types from user data)

### Step 3: Enhanced Labeling Reviews (Updated Heuristics)
We use advanced heuristic rules with 8 categories to label reviews as deceptive or genuine, significantly increasing deceptive samples from ~0.18% to ~28.38%:

#### Deceptive Categories:
1. **Extreme Language**: Reviews with repetitive positive words ("awesome", "amazing", "fantastic") with high stars
2. **Over-Promotion**: Reviews containing phrases like "best ever", "highly recommend" with 5 stars
3. **Short/Generic Text**: Reviews with 1 or 5 stars AND less than 10 words
4. **Time-Based Duplicates**: Reviews posted at similar times with identical text patterns
5. **Unnatural Punctuation/Caps**: Excessive use of exclamation marks, ALL CAPS text, or unusual punctuation
6. **Negative Sentiment with High Stars**: Reviews with negative words but 4-5 stars
7. **Overly Negative with Low Stars**: Reviews with extreme negative language but only 1-2 stars
8. **Keyword Stuffing**: Excessive repetition of positive/negative words (more than 5 occurrences)

#### Genuine Reinforcement:
- Reviews with moderate length (>50 words), positive sentiment (>0.2), and moderate stars (2-4) are reinforced as genuine

### Step 4: Balancing Dataset
We create a balanced dataset by sampling equal numbers of deceptive and genuine reviews to avoid bias in training.

## Machine Learning Models

### Model 1: Multi-Modal Deep Learning Model

#### What is Multi-Modal Learning?
Multi-modal learning combines information from different types of data sources. Here we combine:
- **Text modality**: Review text processed by a language model
- **Behavioral modality**: Numerical features about user behavior

#### Architecture Details

**Text Encoder (DistilBERT)**:
- DistilBERT is a smaller, faster version of BERT (Bidirectional Encoder Representations from Transformers)
- BERT is a pre-trained language model that understands context in text
- DistilBERT converts review text into numerical vectors (embeddings) of 768 dimensions
- We use mean pooling to get a single vector representation of the entire review

**Behavioral Encoder**:
- A neural network that processes the 7 numerical features
- Architecture: Linear(7) → ReLU → Dropout(0.3) → Linear(256) → ReLU → Dropout(0.3)

**Fusion Layer**:
- Combines text embeddings (768 dims) + behavioral embeddings (256 dims) = 1024 dims total
- Uses attention mechanism to learn which parts are more important
- Final classifier: Linear layers with dropout and ReLU activation

**Key Hyperparameters**:
- Hidden dimension: 512
- Learning rate: 2e-5
- Batch size: 16
- Dropout: 0.3-0.4
- Activation: GELU (Gaussian Error Linear Unit)
- No BatchNorm1d (removed for better performance)

#### Training Process
1. Load preprocessed dataset
2. Split into train/validation sets (80/20)
3. Initialize DistilBERT and custom layers
4. Train for 30 epochs with early stopping (patience=3)
5. Use Adam optimizer and binary cross-entropy loss
6. Monitor validation accuracy and stop if no improvement

### Model 2: Random Forest Classifier

#### What is Random Forest?
Random Forest is an ensemble learning method that combines multiple decision trees. Each tree votes on the prediction, and the majority vote wins.

#### How it Works
1. **Bootstrap Sampling**: Creates multiple subsets of the training data
2. **Feature Randomness**: Each tree only sees a random subset of features
3. **Voting**: Final prediction is the most common prediction across all trees

#### Features Used
The Random Forest uses 14 features:
- **User-modifiable features** (11): stars, useful, funny, cool, user_review_count, user_average_stars, is_elite, review_length, user_friends, user_fans, user_compliment_count
- **Derived features** (3): sentiment polarity, number of sentences, average word length

#### Hyperparameters
- Number of trees: 100
- Maximum depth: None (trees can grow fully)
- Minimum samples per leaf: 1
- Random state: 42 (for reproducibility)

## Model Training and Evaluation

### Training Setup
- **Multi-modal**: Currently training on 283,830 balanced samples (141,915 deceptive + 141,915 genuine) from 500,000 reviews
- **Random Forest**: Previously trained on 5,638 samples
- Both models use 80/20 train/validation split
- Current training uses CUDA GPU acceleration for faster processing

### What is CUDA?
CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and programming model that enables dramatic increases in computing performance by harnessing the power of GPU (Graphics Processing Unit). In machine learning:
- **GPU Acceleration**: GPUs have thousands of cores optimized for parallel processing, making them much faster than CPUs for matrix operations common in deep learning
- **Training Speed**: What would take hours on CPU can complete in minutes on GPU
- **Memory**: GPUs have dedicated high-bandwidth memory for faster data transfer
- **Current Setup**: Training is running on CUDA-enabled GPU with batch processing for optimal performance

### Evaluation Metrics

#### Accuracy
- Percentage of correct predictions
- Formula: (True Positives + True Negatives) / Total Samples

#### Precision
- Of all predicted positive cases, how many are actually positive
- Formula: True Positives / (True Positives + False Positives)

#### Recall (Sensitivity)
- Of all actual positive cases, how many did we catch
- Formula: True Positives / (True Positives + False Negatives)

#### F1-Score
- Harmonic mean of precision and recall
- Formula: 2 * (Precision * Recall) / (Precision + Recall)

#### ROC-AUC
- Area Under the Receiver Operating Characteristic curve
- Measures model's ability to distinguish between classes
- Ranges from 0.5 (random) to 1.0 (perfect)

#### Confusion Matrix
A table showing:
- True Positives (correctly identified deceptive)
- True Negatives (correctly identified genuine)
- False Positives (genuine marked as deceptive)
- False Negatives (deceptive marked as genuine)

### Training Results

**Multi-Modal Model (Latest Results - 10k reviews, enhanced heuristics)**:
- **Accuracy**: 83.64%
- **ROC-AUC**: 92.49%
- **Precision**: 84.13% (macro average)
- **Recall**: 83.63% (macro average)
- **F1-Score**: 83.57% (macro average)
- Training completed in ~7 minutes on GPU (6 epochs, early stopping)
- Best validation accuracy: 87.09% at epoch 3
- Confusion Matrix: [[426, 123], [57, 494]] (True Positives/Negatives well balanced)
- Trained on 5,498 balanced samples (2,749 deceptive + 2,749 genuine from 10k reviews)

**Multi-Modal Model (Previous Results - 1.8k samples)**:
- Accuracy: 96.71%
- ROC-AUC: 98.40%
- Trained on CPU for ~4 hours
- Early stopping triggered at epoch 4

**Random Forest Model**:
- Accuracy: 72.25%
- ROC-AUC: 81.22%
- Training time: ~2 minutes

## Streamlit Web Applications

### Multi-Modal App (app.py)
- Runs on http://localhost:8501
- Input: Review text + 7 behavioral feature sliders
- Output: Prediction probability + model metrics display
- Uses the trained multi-modal model for real-time predictions

### Random Forest App (rf_app.py)
- Runs on different port (e.g., 8502)
- Input: Review text + 11 behavioral feature sliders
- Output: Prediction + feature importance visualization
- Uses the trained Random Forest model

### How Apps Work
1. User enters review text and adjusts feature sliders
2. App preprocesses input (tokenizes text, scales features)
3. Model makes prediction
4. Results displayed with confidence scores and metrics

## Technical Implementation Details

### Dependencies
- **torch**: PyTorch deep learning framework
- **transformers**: Hugging Face library for pre-trained models
- **streamlit**: Web app framework
- **pandas**: Data manipulation
- **numpy**: Numerical computing
- **sklearn**: Machine learning utilities
- **json**: Data serialization

### File Structure
- **data_preprocessing_yelp.py**: Data loading and feature extraction with enhanced 8-category heuristics
- **multi_modal_model.py**: Multi-modal model architecture and training (DistilBERT + behavioral features)
- **random_forest_model.py**: Random Forest implementation
- **prediction.py**: Prediction functions for both models
- **app.py**: Multi-modal Streamlit interface
- **rf_app.py**: Random Forest Streamlit interface
- **train_multi_modal.py**: Training script for multi-modal model (currently running on 500k reviews)
- **count_deceptive.py**: Script to count deceptive labels before balancing (28.38% deceptive in 500k reviews)
- **test_heuristics.py**: Small-scale testing of enhanced labeling heuristics
- **dataset_details.txt**: Comprehensive dataset information and statistics
- **requirements.txt**: Python dependencies including textblob for sentiment analysis
- **project_report.txt**: This detailed project documentation

### Key Challenges Solved
1. **Class Imbalance**: Enhanced heuristics increased deceptive samples from 0.18% to 28.38%
2. **Feature Engineering**: Extracted 7 behavioral features + sentiment analysis for better detection
3. **Model Architecture**: Designed attention-based fusion for multi-modal learning with CUDA acceleration
4. **Performance Optimization**: Used DistilBERT instead of full BERT, removed BatchNorm, GPU training
5. **Data Processing**: Efficiently processed 500k+ reviews with memory optimization
6. **Web Deployment**: Created user-friendly interfaces with Streamlit for both models
7. **Labeling Enhancement**: Implemented 8 sophisticated categories for deceptive review detection

## Project Workflow Summary

1. **Data Collection**: Download Yelp Academic Dataset (6.99M reviews, 1.99M users)
2. **Preprocessing**: Load 500k reviews, extract 7 behavioral features, apply 8-category enhanced heuristics
3. **Label Enhancement**: Increased deceptive samples from 0.18% to 28.38% (141,915 deceptive reviews)
4. **Dataset Balancing**: Created balanced training set of 283,830 samples (equal deceptive/genuine)
5. **Model Development**: Implemented multi-modal (DistilBERT + behavioral) and Random Forest architectures
6. **Training**: Currently training multi-modal model on GPU with CUDA acceleration (2-6 hours expected)
7. **Evaluation**: Assess performance using accuracy, precision, recall, F1-score, ROC-AUC, confusion matrix
8. **Deployment**: Create Streamlit apps for real-time predictions with both models
9. **Documentation**: Comprehensive reporting including dataset details, CUDA explanation, and technical specs
10. **Version Control**: Push clean code to GitHub (excluding large model files)

## Technical Specifications

### Hardware Requirements
- **GPU**: NVIDIA GPU with CUDA support (currently using for training)
- **RAM**: Minimum 16GB, recommended 32GB+ for large datasets
- **Storage**: 50GB+ for dataset and model files

### Software Stack
- **Python**: 3.8+
- **PyTorch**: 2.0+ with CUDA support
- **Transformers**: 4.0+ (Hugging Face)
- **CUDA**: 11.8+ (NVIDIA GPU acceleration)
- **Streamlit**: 1.0+ (web interfaces)

### Performance Metrics
- **Data Processing**: 500k reviews processed in ~5-10 minutes
- **Labeling Efficiency**: 28.38% deceptive detection rate with enhanced heuristics
- **Training Speed**: 2-6 hours for 283k samples on GPU (vs hours/days on CPU)
- **Model Size**: ~500MB for multi-modal model (DistilBERT + custom layers)

## Learning Outcomes

This project demonstrates:
- **NLP Techniques**: Text preprocessing, embeddings, language models, sentiment analysis
- **Deep Learning**: Neural networks, attention mechanisms, transfer learning, CUDA acceleration
- **Traditional ML**: Ensemble methods, feature importance, hyperparameter tuning
- **Data Engineering**: Large-scale data processing, feature extraction, heuristic labeling
- **Web Development**: Interactive applications with Streamlit, real-time predictions
- **MLOps**: Model training, evaluation, deployment, GPU optimization
- **Research Methods**: Academic dataset handling, experimental design, performance analysis

## Current Status
- ✅ Enhanced heuristics implemented (27.49% deceptive detection on 10k sample)
- ✅ Dataset balanced (5,498 samples from 10k reviews for testing)
- ✅ Multi-modal model successfully trained on GPU (83.64% accuracy, 92.49% ROC-AUC)
- ✅ Random Forest model previously trained (72.25% accuracy)
- ✅ Streamlit interfaces ready for deployment
- ✅ Comprehensive documentation updated with latest results
- ✅ Model metrics and training history saved

## Future Improvements
- Increase dataset size to 1M+ reviews for better generalization
- Experiment with other language models (RoBERTa, XLNet, ELECTRA)
- Add temporal features (review timing patterns, burst detection)
- Implement online learning for continuous model updates
- Add explainability features (SHAP, LIME) to understand model decisions
- Deploy models as REST APIs for production use
- Add A/B testing framework for model comparison
- Implement automated retraining pipelines

---

This report provides a complete overview of the fake review detection project, from data collection to deployment. All technical terms are explained, and the step-by-step process is detailed to help with presentations and Q&A sessions.
